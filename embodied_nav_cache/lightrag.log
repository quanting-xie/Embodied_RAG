2024-10-21 22:12:34,864 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-21 22:12:34,864 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7f2bc6788b80>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7f2bc6788700>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7f2bc675e050>

2024-10-21 22:12:34,865 - lightrag - INFO - Load KV full_docs with 0 data
2024-10-21 22:12:34,865 - lightrag - INFO - Load KV text_chunks with 0 data
2024-10-21 22:12:34,865 - lightrag - INFO - Load KV llm_response_cache with 0 data
2024-10-21 22:13:14,647 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-21 22:13:14,648 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7f8abd2e8c10>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7f8abd2e8790>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7f8abd2ba0e0>

2024-10-21 22:13:14,648 - lightrag - INFO - Load KV full_docs with 0 data
2024-10-21 22:13:14,648 - lightrag - INFO - Load KV text_chunks with 0 data
2024-10-21 22:13:14,648 - lightrag - INFO - Load KV llm_response_cache with 0 data
2024-10-22 10:48:06,900 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-22 10:48:06,900 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7f0ffa640af0>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7f0ffa640670>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7f0ffa8ddfc0>

2024-10-22 10:48:06,900 - lightrag - INFO - Load KV full_docs with 0 data
2024-10-22 10:48:06,900 - lightrag - INFO - Load KV text_chunks with 0 data
2024-10-22 10:48:06,900 - lightrag - INFO - Load KV llm_response_cache with 0 data
2024-10-22 10:52:52,764 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-22 10:52:52,765 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7f3283cd0c10>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7f3283cd0790>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7f3283ca20e0>

2024-10-22 10:52:52,765 - lightrag - INFO - Load KV full_docs with 0 data
2024-10-22 10:52:52,765 - lightrag - INFO - Load KV text_chunks with 0 data
2024-10-22 10:52:52,765 - lightrag - INFO - Load KV llm_response_cache with 0 data
2024-10-22 10:57:38,966 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-22 10:57:38,966 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7fb120d84af0>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7fb120d84670>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7fb120d51fc0>

2024-10-22 10:57:38,967 - lightrag - INFO - Load KV full_docs with 0 data
2024-10-22 10:57:38,967 - lightrag - INFO - Load KV text_chunks with 0 data
2024-10-22 10:57:38,967 - lightrag - INFO - Load KV llm_response_cache with 0 data
2024-10-22 11:00:40,440 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-22 11:00:40,440 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7f8355088c10>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7f8355088790>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7f835505e0e0>

2024-10-22 11:00:40,440 - lightrag - INFO - Load KV full_docs with 0 data
2024-10-22 11:00:40,440 - lightrag - INFO - Load KV text_chunks with 0 data
2024-10-22 11:00:40,440 - lightrag - INFO - Load KV llm_response_cache with 0 data
2024-10-22 11:02:05,222 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-22 11:02:05,222 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7f832de5cc10>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7f832de5c790>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7f832de360e0>

2024-10-22 11:02:05,222 - lightrag - INFO - Load KV full_docs with 0 data
2024-10-22 11:02:05,222 - lightrag - INFO - Load KV text_chunks with 0 data
2024-10-22 11:02:05,222 - lightrag - INFO - Load KV llm_response_cache with 0 data
2024-10-22 11:04:04,752 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-22 11:04:04,753 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7f9745ffcc10>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7f9745ffc790>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7f9745fd60e0>

2024-10-22 11:04:04,753 - lightrag - INFO - Load KV full_docs with 0 data
2024-10-22 11:04:04,753 - lightrag - INFO - Load KV text_chunks with 0 data
2024-10-22 11:04:04,753 - lightrag - INFO - Load KV llm_response_cache with 0 data
2024-10-22 11:05:25,744 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-22 11:05:25,745 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7f691044cc10>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7f691044c790>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7f69106ee0e0>

2024-10-22 11:05:25,745 - lightrag - INFO - Load KV full_docs with 0 data
2024-10-22 11:05:25,745 - lightrag - INFO - Load KV text_chunks with 0 data
2024-10-22 11:05:25,745 - lightrag - INFO - Load KV llm_response_cache with 0 data
2024-10-22 11:14:04,101 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-22 11:14:04,102 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7f824b6e4c10>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7f824b6e4790>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7f824b6ba0e0>

2024-10-22 11:14:04,104 - lightrag - INFO - Load KV full_docs with 0 data
2024-10-22 11:14:04,104 - lightrag - INFO - Load KV text_chunks with 0 data
2024-10-22 11:14:04,104 - lightrag - INFO - Load KV llm_response_cache with 0 data
2024-10-22 11:14:58,549 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-22 11:14:58,550 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7fecb3684a60>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7fecb36845e0>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7fecb3925f30>

2024-10-22 11:14:58,552 - lightrag - INFO - Load KV full_docs with 0 data
2024-10-22 11:14:58,552 - lightrag - INFO - Load KV text_chunks with 0 data
2024-10-22 11:14:58,552 - lightrag - INFO - Load KV llm_response_cache with 0 data
2024-10-22 11:24:15,464 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-22 11:24:15,464 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7f5067f8ca60>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7f5067f8c5e0>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7f5067f5df30>

2024-10-22 11:24:15,466 - lightrag - INFO - Load KV full_docs with 0 data
2024-10-22 11:24:15,466 - lightrag - INFO - Load KV text_chunks with 0 data
2024-10-22 11:24:15,466 - lightrag - INFO - Load KV llm_response_cache with 0 data
2024-10-22 11:25:35,643 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-22 11:25:35,644 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7f39f889caf0>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7f39f889c670>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7f39f886dfc0>

2024-10-22 11:25:35,645 - lightrag - INFO - Load KV full_docs with 0 data
2024-10-22 11:25:35,646 - lightrag - INFO - Load KV text_chunks with 0 data
2024-10-22 11:25:35,646 - lightrag - INFO - Load KV llm_response_cache with 0 data
2024-10-22 11:26:11,905 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-22 11:26:11,906 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7f1d4481ca60>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7f1d4481c5e0>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7f1d447edf30>

2024-10-22 11:26:11,908 - lightrag - INFO - Load KV full_docs with 0 data
2024-10-22 11:26:11,908 - lightrag - INFO - Load KV text_chunks with 0 data
2024-10-22 11:26:11,908 - lightrag - INFO - Load KV llm_response_cache with 0 data
2024-10-22 11:27:49,518 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-22 11:27:49,519 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7fa549580a60>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7fa5495805e0>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7fa549575f30>

2024-10-22 11:27:49,521 - lightrag - INFO - Load KV full_docs with 0 data
2024-10-22 11:27:49,521 - lightrag - INFO - Load KV text_chunks with 0 data
2024-10-22 11:27:49,521 - lightrag - INFO - Load KV llm_response_cache with 0 data
2024-10-22 11:29:15,085 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-22 11:29:15,086 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7f43ffb98a60>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7f43ffb985e0>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7f43ffb8df30>

2024-10-22 11:29:15,087 - lightrag - INFO - Load KV full_docs with 0 data
2024-10-22 11:29:15,088 - lightrag - INFO - Load KV text_chunks with 0 data
2024-10-22 11:29:15,088 - lightrag - INFO - Load KV llm_response_cache with 0 data
2024-10-22 11:31:08,559 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-22 11:31:08,559 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7f9ad8db8af0>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7f9ad8db8670>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7f9ad9059fc0>

2024-10-22 11:31:08,561 - lightrag - INFO - Load KV full_docs with 0 data
2024-10-22 11:31:08,562 - lightrag - INFO - Load KV text_chunks with 0 data
2024-10-22 11:31:08,562 - lightrag - INFO - Load KV llm_response_cache with 0 data
2024-10-22 11:31:08,602 - lightrag - INFO - [New Docs] inserting 1462 docs
2024-10-22 11:31:09,485 - lightrag - INFO - [New Chunks] inserting 1462 chunks
2024-10-22 11:31:09,485 - lightrag - INFO - Inserting 1462 vectors to chunks
2024-10-22 11:31:18,544 - lightrag - INFO - [Entity Extraction]...
2024-10-22 11:40:10,395 - lightrag - INFO - Writing graph with 0 nodes, 0 edges
2024-10-22 14:52:18,636 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-22 14:52:18,636 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7fdeb675cca0>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7fdeb675c820>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7fdeb672e170>

2024-10-22 14:52:18,639 - lightrag - INFO - Load KV full_docs with 0 data
2024-10-22 14:52:18,639 - lightrag - INFO - Load KV text_chunks with 0 data
2024-10-22 14:52:18,647 - lightrag - INFO - Load KV llm_response_cache with 1820 data
2024-10-22 14:52:18,648 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 0 nodes, 0 edges
2024-10-22 15:27:49,541 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-22 15:27:49,541 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7f00e6ae0ca0>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7f00e6ae0820>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7f00e6ab2170>

2024-10-22 15:27:49,544 - lightrag - INFO - Load KV full_docs with 0 data
2024-10-22 15:27:49,544 - lightrag - INFO - Load KV text_chunks with 0 data
2024-10-22 15:27:49,560 - lightrag - INFO - Load KV llm_response_cache with 1820 data
2024-10-22 15:27:49,561 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 0 nodes, 0 edges
2024-10-22 15:27:49,612 - lightrag - INFO - [New Docs] inserting 143 docs
2024-10-22 15:27:54,244 - lightrag - INFO - [New Chunks] inserting 143 chunks
2024-10-22 15:27:54,244 - lightrag - INFO - Inserting 143 vectors to chunks
2024-10-22 15:27:56,041 - lightrag - INFO - [Entity Extraction]...
2024-10-22 15:28:10,659 - lightrag - INFO - Writing graph with 0 nodes, 0 edges
2024-10-26 11:00:06,889 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-26 11:00:06,889 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x16cdcd3f0>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x16cdcce50>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x16cdba7a0>

2024-10-26 11:00:06,890 - lightrag - INFO - Load KV full_docs with 0 data
2024-10-26 11:00:06,890 - lightrag - INFO - Load KV text_chunks with 0 data
2024-10-26 11:00:06,902 - lightrag - INFO - Load KV llm_response_cache with 1880 data
2024-10-26 11:00:06,903 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 0 nodes, 0 edges
2024-10-26 11:00:56,800 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-26 11:00:56,801 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x16b949480>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x16b948ee0>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x16b93e830>

2024-10-26 11:00:56,801 - lightrag - INFO - Load KV full_docs with 0 data
2024-10-26 11:00:56,801 - lightrag - INFO - Load KV text_chunks with 0 data
2024-10-26 11:00:56,813 - lightrag - INFO - Load KV llm_response_cache with 1880 data
2024-10-26 11:00:56,814 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 0 nodes, 0 edges
2024-10-26 11:01:13,722 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-26 11:01:13,723 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x16e7d5510>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x16e7d4f70>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x16e7ca8c0>

2024-10-26 11:01:13,723 - lightrag - INFO - Load KV full_docs with 0 data
2024-10-26 11:01:13,723 - lightrag - INFO - Load KV text_chunks with 0 data
2024-10-26 11:01:13,735 - lightrag - INFO - Load KV llm_response_cache with 1880 data
2024-10-26 11:01:13,736 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 0 nodes, 0 edges
2024-10-26 11:01:13,821 - lightrag - INFO - [New Docs] inserting 50 docs
2024-10-26 11:01:15,689 - lightrag - INFO - [New Chunks] inserting 50 chunks
2024-10-26 11:01:15,689 - lightrag - INFO - Inserting 50 vectors to chunks
2024-10-26 11:01:17,098 - lightrag - INFO - [Entity Extraction]...
2024-10-26 11:01:49,130 - lightrag - DEBUG - Trigger summary: "DRONE"
2024-10-26 11:01:49,153 - lightrag - DEBUG - Trigger summary: "CHAIR_CAFECHARCOALGREY10"
2024-10-26 11:01:52,639 - lightrag - INFO - Inserting 77 vectors to entities
2024-10-26 11:01:53,986 - lightrag - INFO - Inserting 160 vectors to relationships
2024-10-26 11:01:56,400 - lightrag - INFO - Writing graph with 78 nodes, 160 edges
2024-10-26 11:05:39,332 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-26 11:05:39,332 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x16aadd3f0>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x16aadce50>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x169a627a0>

2024-10-26 11:05:39,333 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-26 11:05:39,334 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-26 11:05:39,346 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-26 11:05:39,353 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-26 11:14:39,205 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-26 11:14:39,205 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x16bed1480>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x16bed0ee0>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x16bec6830>

2024-10-26 11:14:39,206 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-26 11:14:39,206 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-26 11:14:39,219 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-26 11:14:39,226 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-26 11:28:45,907 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-26 11:28:45,907 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x1692c1480>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x1692c0ee0>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x16929e830>

2024-10-26 11:28:45,908 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-26 11:28:45,909 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-26 11:28:45,921 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-26 11:28:45,928 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-26 11:30:13,125 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-26 11:30:13,126 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x16a11d480>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x16a11cee0>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x16a0f6830>

2024-10-26 11:30:13,127 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-26 11:30:13,127 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-26 11:30:13,140 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-26 11:30:13,146 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-26 20:31:09,466 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-26 20:31:09,467 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7f7c56f34e50>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7f7c56f349d0>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7f7c56f2a320>

2024-10-26 20:31:09,467 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-26 20:31:09,467 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-26 20:31:09,479 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-26 20:31:09,483 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-26 20:31:50,651 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-26 20:31:50,651 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7f3714320e50>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7f37143209d0>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7f3714316320>

2024-10-26 20:31:50,651 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-26 20:31:50,652 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-26 20:31:50,661 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-26 20:31:50,665 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-26 20:33:16,990 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-26 20:33:16,990 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7fedd7e78f70>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7fedd7e78af0>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7fedd7e6a440>

2024-10-26 20:33:16,991 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-26 20:33:16,991 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-26 20:33:17,003 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-26 20:33:17,007 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-26 20:33:56,944 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-26 20:33:56,944 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7f2913854f70>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7f2913854af0>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7f291384a440>

2024-10-26 20:33:56,944 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-26 20:33:56,945 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-26 20:33:56,955 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-26 20:33:56,960 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-26 20:35:31,303 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-26 20:35:31,303 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7f1683920e50>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7f16839209d0>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7f1683912320>

2024-10-26 20:35:31,304 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-26 20:35:31,304 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-26 20:35:31,314 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-26 20:35:31,319 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-26 20:36:11,159 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-26 20:36:11,159 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7fa51e7a8e50>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7fa51e7a89d0>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7fa51e79a320>

2024-10-26 20:36:11,160 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-26 20:36:11,160 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-26 20:36:11,170 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-26 20:36:11,174 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-26 20:39:44,137 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-26 20:39:44,137 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7f40d404ce50>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7f40d404c9d0>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7f40d4042320>

2024-10-26 20:39:44,138 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-26 20:39:44,138 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-26 20:39:44,156 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-26 20:39:44,162 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-26 20:51:23,815 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-26 20:51:23,816 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7fb819128e50>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7fb8191289d0>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7fb81911a320>

2024-10-26 20:51:23,816 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-26 20:51:23,816 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-26 20:51:23,827 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-26 20:51:23,831 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-26 20:52:44,015 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-26 20:52:44,016 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7f757d0b8e50>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7f757d0b89d0>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7f757d0aa320>

2024-10-26 20:52:44,016 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-26 20:52:44,017 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-26 20:52:44,038 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-26 20:52:44,042 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-26 20:57:40,451 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-26 20:57:40,451 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7fbf4fce4ee0>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7fbf4fce4a60>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7fbf4fcda3b0>

2024-10-26 20:57:40,451 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-26 20:57:40,452 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-26 20:57:40,470 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-26 20:57:40,477 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-26 20:59:11,471 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-26 20:59:11,472 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7f6798eb8e50>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7f6798eb89d0>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7f6798eae320>

2024-10-26 20:59:11,472 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-26 20:59:11,473 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-26 20:59:11,491 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-26 20:59:11,496 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-26 21:00:09,980 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-26 21:00:09,981 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7fa043df0e50>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7fa043df09d0>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7fa043de6320>

2024-10-26 21:00:09,981 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-26 21:00:09,982 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-26 21:00:09,996 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-26 21:00:10,001 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-26 21:00:58,351 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-26 21:00:58,351 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7f0ffa6d4e50>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7f0ffa6d49d0>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7f0ffa6ca320>

2024-10-26 21:00:58,352 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-26 21:00:58,352 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-26 21:00:58,370 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-26 21:00:58,374 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-26 21:01:48,873 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-26 21:01:48,874 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7f92c234ce50>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7f92c234c9d0>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7f92c233e320>

2024-10-26 21:01:48,874 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-26 21:01:48,874 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-26 21:01:48,886 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-26 21:01:48,890 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-26 21:03:11,675 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-26 21:03:11,675 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7f9eb3590e50>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7f9eb35909d0>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7f9eb3582320>

2024-10-26 21:03:11,676 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-26 21:03:11,676 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-26 21:03:11,692 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-26 21:03:11,698 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-26 21:03:47,432 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-26 21:03:47,432 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7f6b14cc4e50>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7f6b14cc49d0>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7f6b14cb6320>

2024-10-26 21:03:47,433 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-26 21:03:47,433 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-26 21:03:47,447 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-26 21:03:47,452 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-26 21:05:52,435 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-26 21:05:52,436 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7fe8c5c84f70>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7fe8c5c84af0>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7fe8c5c7a440>

2024-10-26 21:05:52,436 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-26 21:05:52,436 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-26 21:05:52,456 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-26 21:05:52,462 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-26 21:08:27,895 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-26 21:08:27,895 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7f201fed9000>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7f201fed8b80>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7f201feca4d0>

2024-10-26 21:08:27,896 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-26 21:08:27,896 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-26 21:08:27,918 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-26 21:08:27,922 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-26 21:09:36,234 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-26 21:09:36,234 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7f9237665000>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7f9237664b80>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7f923765a4d0>

2024-10-26 21:09:36,235 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-26 21:09:36,235 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-26 21:09:36,249 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-26 21:09:36,255 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-26 21:09:58,945 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-26 21:09:58,945 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7f9f5ddd5000>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7f9f5ddd4b80>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7f9f5ddc64d0>

2024-10-26 21:09:58,945 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-26 21:09:58,945 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-26 21:09:58,957 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-26 21:09:58,962 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-26 21:12:23,911 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-26 21:12:23,911 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7fde1b711000>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7fde1b710b80>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7fde1b7024d0>

2024-10-26 21:12:23,912 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-26 21:12:23,912 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-26 21:12:23,924 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-26 21:12:23,929 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-26 21:12:38,562 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-26 21:12:38,562 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7f42b74b9000>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7f42b74b8b80>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7f42b74ae4d0>

2024-10-26 21:12:38,563 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-26 21:12:38,563 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-26 21:12:38,576 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-26 21:12:38,580 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-26 21:16:57,182 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-26 21:16:57,183 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7f2380080f70>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7f2380080af0>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7f2380076440>

2024-10-26 21:16:57,183 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-26 21:16:57,184 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-26 21:16:57,202 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-26 21:16:57,208 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-26 21:20:15,273 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-26 21:20:15,273 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7f28ba094f70>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7f28ba094af0>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7f28ba082440>

2024-10-26 21:20:15,274 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-26 21:20:15,274 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-26 21:20:15,288 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-26 21:20:15,292 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-26 21:20:31,058 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-26 21:20:31,058 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7f8a89fcd000>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7f8a89fccb80>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7f8a89fc24d0>

2024-10-26 21:20:31,059 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-26 21:20:31,059 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-26 21:20:31,075 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-26 21:20:31,079 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-26 21:20:44,873 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-26 21:20:44,873 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7f0c39385000>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7f0c39384b80>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7f0c3937a4d0>

2024-10-26 21:20:44,874 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-26 21:20:44,874 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-26 21:20:44,886 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-26 21:20:44,890 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-26 21:23:26,365 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-26 21:23:26,366 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7f56c0088f70>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7f56c0088af0>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7f56c007a440>

2024-10-26 21:23:26,366 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-26 21:23:26,366 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-26 21:23:26,378 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-26 21:23:26,382 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-26 21:23:54,639 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-26 21:23:54,639 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7ff599f8d000>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7ff599f8cb80>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7ff599f824d0>

2024-10-26 21:23:54,639 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-26 21:23:54,640 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-26 21:23:54,657 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-26 21:23:54,663 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-26 21:24:24,988 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-26 21:24:24,988 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7f41345f1000>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7f41345f0b80>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7f41345e64d0>

2024-10-26 21:24:24,988 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-26 21:24:24,989 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-26 21:24:25,003 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-26 21:24:25,009 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-26 21:24:54,476 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-26 21:24:54,477 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7f9e95c35000>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7f9e95c34b80>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7f9e95c2a4d0>

2024-10-26 21:24:54,477 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-26 21:24:54,478 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-26 21:24:54,493 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-26 21:24:54,497 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-26 21:30:04,290 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-26 21:30:04,291 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7fc40833cdc0>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7fc40833c940>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7fc408332290>

2024-10-26 21:30:04,291 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-26 21:30:04,292 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-26 21:30:04,310 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-26 21:30:04,315 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-26 21:30:50,714 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-26 21:30:50,715 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7fc407138e50>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7fc4071389d0>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7fc40712e320>

2024-10-26 21:30:50,715 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-26 21:30:50,715 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-26 21:30:50,726 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-26 21:30:50,730 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-26 21:31:22,531 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-26 21:31:22,531 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7fa8816e8e50>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7fa8816e89d0>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7fa8816de320>

2024-10-26 21:31:22,532 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-26 21:31:22,532 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-26 21:31:22,550 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-26 21:31:22,555 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-26 21:31:51,715 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-26 21:31:51,715 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7fb5e9b54e50>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7fb5e9b549d0>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7fb5e9b4a320>

2024-10-26 21:31:51,716 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-26 21:31:51,716 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-26 21:31:51,729 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-26 21:31:51,734 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-26 21:32:28,456 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-26 21:32:28,456 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7f64e997ce50>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7f64e997c9d0>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7f64e9972320>

2024-10-26 21:32:28,457 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-26 21:32:28,457 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-26 21:32:28,479 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-26 21:32:28,484 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-26 21:34:20,605 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-26 21:34:20,606 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7f055326ce50>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7f055326c9d0>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7f055325e320>

2024-10-26 21:34:20,606 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-26 21:34:20,606 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-26 21:34:20,617 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-26 21:34:20,621 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-26 21:34:54,697 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-26 21:34:54,697 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7f89f6408e50>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7f89f64089d0>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7f89f63fe320>

2024-10-26 21:34:54,697 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-26 21:34:54,698 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-26 21:34:54,708 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-26 21:34:54,712 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-27 13:50:50,116 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-27 13:50:50,116 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7fe48304ce50>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7fe48304c9d0>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7fe483042320>

2024-10-27 13:50:50,116 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-27 13:50:50,117 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-27 13:50:50,142 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-27 13:50:50,146 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-27 13:52:02,965 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-27 13:52:02,965 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7f4786c00e50>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7f4786c009d0>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7f4786bf6320>

2024-10-27 13:52:02,966 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-27 13:52:02,966 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-27 13:52:02,982 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-27 13:52:02,988 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-27 13:53:23,729 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-27 13:53:23,729 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7fbc78558ee0>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7fbc78558a60>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7fbc7854a3b0>

2024-10-27 13:53:23,730 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-27 13:53:23,730 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-27 13:53:23,742 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-27 13:53:23,748 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-27 13:56:13,525 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-27 13:56:13,526 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7f6ee72c4d30>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7f6ee72c48b0>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7f6ee72ba200>

2024-10-27 13:56:13,526 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-27 13:56:13,527 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-27 13:56:13,540 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-27 13:56:13,545 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-27 13:56:47,917 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-27 13:56:47,917 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7f4b2a46cdc0>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7f4b2a46c940>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7f4b2a45e290>

2024-10-27 13:56:47,918 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-27 13:56:47,918 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-27 13:56:47,933 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-27 13:56:47,939 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-27 13:59:06,990 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-27 13:59:06,990 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7fa727ee0dc0>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7fa727ee0940>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7fa727ed6290>

2024-10-27 13:59:06,991 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-27 13:59:06,991 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-27 13:59:07,005 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-27 13:59:07,009 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-27 14:00:33,074 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-27 14:00:33,074 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7f9cf0a80dc0>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7f9cf0a80940>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7f9cf0a7a290>

2024-10-27 14:00:33,075 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-27 14:00:33,075 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-27 14:00:33,086 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-27 14:00:33,090 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-27 15:18:55,844 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-27 15:18:55,844 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7f92a3f28dc0>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7f92a3f28940>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7f92a3f1a290>

2024-10-27 15:18:55,844 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-27 15:18:55,844 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-27 15:18:55,855 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-27 15:18:55,859 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-27 15:23:14,043 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-27 15:23:14,044 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7f97b8ab4dc0>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7f97b8ab4940>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7f97b8aa6290>

2024-10-27 15:23:14,044 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-27 15:23:14,045 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-27 15:23:14,060 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-27 15:23:14,064 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-27 16:45:31,763 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-27 16:45:31,763 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7f14b99fcdc0>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7f14b99fc940>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7f14b99ee290>

2024-10-27 16:45:31,763 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-27 16:45:31,764 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-27 16:45:31,775 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-27 16:45:31,781 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-27 16:45:50,015 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-27 16:45:50,015 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7fa4db698dc0>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7fa4db698940>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7fa4db68a290>

2024-10-27 16:45:50,016 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-27 16:45:50,016 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-27 16:45:50,028 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-27 16:45:50,032 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-27 16:46:16,488 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-27 16:46:16,489 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7f2406020dc0>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7f2406020940>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7f2406012290>

2024-10-27 16:46:16,489 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-27 16:46:16,489 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-27 16:46:16,500 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-27 16:46:16,506 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-27 16:46:42,899 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-27 16:46:42,900 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7f0f20b84dc0>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7f0f20b84940>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7f0f20b76290>

2024-10-27 16:46:42,901 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-27 16:46:42,901 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-27 16:46:42,916 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-27 16:46:42,921 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-27 16:48:00,624 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-27 16:48:00,625 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7fb699150e50>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7fb6991509d0>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7fb69913e320>

2024-10-27 16:48:00,625 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-27 16:48:00,626 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-27 16:48:00,643 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-27 16:48:00,648 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-27 16:51:02,281 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-27 16:51:02,282 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7f87221acd30>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7f87221ac8b0>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7f872219e200>

2024-10-27 16:51:02,282 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-27 16:51:02,282 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-27 16:51:02,294 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-27 16:51:02,298 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-27 16:53:34,323 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-27 16:53:34,323 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7f8d0d834dc0>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7f8d0d834940>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7f8d0d826290>

2024-10-27 16:53:34,324 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-27 16:53:34,324 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-27 16:53:34,337 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-27 16:53:34,341 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-27 17:02:52,338 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-27 17:02:52,338 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7f6d5fa80dc0>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7f6d5fa80940>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7f6d5fa76290>

2024-10-27 17:02:52,339 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-27 17:02:52,339 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-27 17:02:52,358 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-27 17:02:52,363 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-27 17:13:55,068 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-27 17:13:55,069 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7f6f6ca2ce50>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7f6f6ca2c9d0>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7f6f6ca22320>

2024-10-27 17:13:55,069 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-27 17:13:55,069 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-27 17:13:55,083 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-27 17:13:55,088 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-28 10:11:04,743 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-28 10:11:04,743 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7fdb7d398e50>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7fdb7d3989d0>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7fdb7d392320>

2024-10-28 10:11:04,744 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-28 10:11:04,744 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-28 10:11:04,755 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-28 10:11:04,759 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-28 10:15:35,984 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-28 10:15:35,984 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7fa24bf88e50>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7fa24bf889d0>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7fa24bf7a320>

2024-10-28 10:15:35,985 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-28 10:15:35,985 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-28 10:15:35,997 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-28 10:15:36,002 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-28 10:19:17,777 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-28 10:19:17,777 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7f279c590e50>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7f279c5909d0>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7f279c586320>

2024-10-28 10:19:17,778 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-28 10:19:17,778 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-28 10:19:17,795 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-28 10:19:17,800 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-28 10:19:51,340 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-28 10:19:51,341 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7f94eb084e50>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7f94eb0849d0>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7f94eb07e320>

2024-10-28 10:19:51,341 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-28 10:19:51,342 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-28 10:19:51,359 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-28 10:19:51,364 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-28 10:21:48,194 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-28 10:21:48,195 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7f7cc9180e50>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7f7cc91809d0>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7f7cc9176320>

2024-10-28 10:21:48,195 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-28 10:21:48,196 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-28 10:21:48,209 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-28 10:21:48,213 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-28 10:22:15,737 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-28 10:22:15,737 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7f3119588e50>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7f31195889d0>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7f311957a320>

2024-10-28 10:22:15,737 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-28 10:22:15,738 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-28 10:22:15,754 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-28 10:22:15,760 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
2024-10-28 10:24:16,027 - lightrag - INFO - Logger initialized for working directory: ./embodied_nav_cache
2024-10-28 10:24:16,027 - lightrag - DEBUG - LightRAG init with param:
  working_dir = ./embodied_nav_cache,
  chunk_token_size = 1200,
  chunk_overlap_token_size = 100,
  tiktoken_model_name = gpt-4o-mini,
  entity_extract_max_gleaning = 1,
  entity_summary_to_max_tokens = 500,
  node_embedding_algorithm = node2vec,
  node2vec_params = {'dimensions': 1536, 'num_walks': 10, 'walk_length': 40, 'window_size': 2, 'iterations': 3, 'random_seed': 3},
  embedding_func = {'embedding_dim': 1536, 'max_token_size': 8192, 'func': <function openai_embedding at 0x7f6b0d130e50>},
  embedding_batch_num = 32,
  embedding_func_max_async = 16,
  llm_model_func = <function gpt_4o_mini_complete at 0x7f6b0d1309d0>,
  llm_model_name = meta-llama/Llama-3.2-1B-Instruct,
  llm_model_max_token_size = 32768,
  llm_model_max_async = 16,
  key_string_value_json_storage_cls = <class 'lightrag.storage.JsonKVStorage'>,
  vector_db_storage_cls = <class 'lightrag.storage.NanoVectorDBStorage'>,
  vector_db_storage_cls_kwargs = {},
  graph_storage_cls = <class 'lightrag.storage.NetworkXStorage'>,
  enable_llm_cache = True,
  addon_params = {},
  convert_response_to_json_func = <function convert_response_to_json at 0x7f6b0d126320>

2024-10-28 10:24:16,027 - lightrag - INFO - Load KV full_docs with 50 data
2024-10-28 10:24:16,028 - lightrag - INFO - Load KV text_chunks with 50 data
2024-10-28 10:24:16,044 - lightrag - INFO - Load KV llm_response_cache with 1982 data
2024-10-28 10:24:16,048 - lightrag - INFO - Loaded graph from ./embodied_nav_cache/graph_chunk_entity_relation.graphml with 78 nodes, 160 edges
